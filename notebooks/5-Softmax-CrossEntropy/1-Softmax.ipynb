{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc794b85",
   "metadata": {},
   "source": [
    "## Softmax\n",
    "(based on a tutorial by Python Engineer in Youtube)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcf1776",
   "metadata": {},
   "source": [
    "**Softmax**\n",
    "\n",
    "The softmax function, also known as softargmax or normalized exponential function is a generalization of the logistic function to multiple dimensions. It is used in multinomial logistic regression and is often used as the last activation function of a neural network to normalize the output of a network to a probability distribution over predicted output classes.\n",
    "\n",
    "The softmax function takes as input a vector z of K real numbers, and normalizes it into a probability distribution consisting of K probabilities proportional to the exponentials of the input numbers. That is, prior to applying softmax, some vector components could be negative, or greater than one; and might not sum to 1; but after applying softmax, each component will be in the interval (0,1), and the components will add up to 1, so that they can be interpreted as probabilities. Furthermore, the larger input components will correspond to larger probabilities.\n",
    "\n",
    "The standard (unit) softmax function:\n",
    "![title](img/softmax.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad12da6",
   "metadata": {},
   "source": [
    "source: wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82237239",
   "metadata": {},
   "source": [
    "**Softmax Implementation with Numpy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e036a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f81dfb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return np.exp(x)/np.sum(np.exp(x), axis=0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dee04dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax numpy [0.65900114 0.24243297 0.09856589]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([2.0, 1.0, 0.1])\n",
    "outputs = softmax(x)\n",
    "print('softmax numpy', outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2aa31f4",
   "metadata": {},
   "source": [
    "**Softmax Implementation with Pytorch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a73693ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.6590, 0.2424, 0.0986])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([2.0, 1.0, 0.1])\n",
    "outputs = torch.softmax(x, dim=0) #we want to compute it based on the first axis\n",
    "print(outputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
