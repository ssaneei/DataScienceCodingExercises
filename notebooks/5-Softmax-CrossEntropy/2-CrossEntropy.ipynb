{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc794b85",
   "metadata": {},
   "source": [
    "## Cross-Entropy\n",
    "(based on a tutorial by Python Engineer in Youtube)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcf1776",
   "metadata": {},
   "source": [
    "**Cross Entropy**\n",
    "\n",
    "\n",
    "![title](img/soft-CE.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad12da6",
   "metadata": {},
   "source": [
    "source: https://levelup.gitconnected.com/killer-combo-softmax-and-cross-entropy-5907442f60ba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1ebf91",
   "metadata": {},
   "source": [
    "And it's formula is:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a180ea",
   "metadata": {},
   "source": [
    "![title](img/formula.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42633389",
   "metadata": {},
   "source": [
    "source: https://levelup.gitconnected.com/grokking-the-cross-entropy-loss-cda6eb9ec307"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82237239",
   "metadata": {},
   "source": [
    "**Cross Entropy Implementation with Numpy**\n",
    "\n",
    "Considering a problem in which we have 3 classes and we show them with a one-hot vector:\n",
    "so class 0 [1 0 0]\n",
    "class 1    [0 1 0]\n",
    "class 2    [0 0 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0e036a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f81dfb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(y_gold, y_predicted):\n",
    "    loss = -np.sum(y_gold * np.log(y_predicted))\n",
    "    return loss\n",
    "    # we can devide it by number of samples to normalize it too.\n",
    "    # loss/ float(y_predicted.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dee04dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss1:0.357\n",
      "Loss2:2.303\n"
     ]
    }
   ],
   "source": [
    "# y must be one hot encoded\n",
    "Y = np.array([1, 0, 0])\n",
    "\n",
    "Y_pred_good = np.array([0.7, 0.2, 0.1])\n",
    "Y_pred_bad = np.array([0.1, 0.3, 0.6])\n",
    "\n",
    "l1 = cross_entropy(Y, Y_pred_good)\n",
    "l2 = cross_entropy(Y, Y_pred_bad)\n",
    "\n",
    "print(f'Loss1:{l1:.3f}')\n",
    "print(f'Loss2:{l2:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2aa31f4",
   "metadata": {},
   "source": [
    "**Cross Entropy Implementation with Pytorch**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a73693ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1ceaa8",
   "metadata": {},
   "source": [
    "    Attention: nn.CrossEntropyLoss applies nn.LogSoftmax + nn.NLLLoss (Negative Log Likelihood Loss)\n",
    "    => no need for Softmax in the last layer!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac37634",
   "metadata": {},
   "source": [
    "    Attention2: Y has class labels => not one-hot, just put the correct class labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162d606d",
   "metadata": {},
   "source": [
    "    Attention3: Y_pred has raw scores (logits) => no Softmax!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "59fc95b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4170299470424652\n",
      "1.840616226196289\n"
     ]
    }
   ],
   "source": [
    "Y = torch.tensor([0])     #actual lables\n",
    "\n",
    "#for y_pred_good we must be careful about the size. \n",
    "#n_samples x n_classes = 1 x 3 \n",
    "#so it must be an array of arrays\n",
    "Y_pred_good = torch.tensor([[2.0, 1.0, 0.1]])    #These are the raw values and we did'nt apply the softmax\n",
    "Y_pred_bad = torch.tensor([[0.5, 2.0, 0.3]]) \n",
    "\n",
    "#compute the loss\n",
    "l1 = loss(Y_pred_good, Y)\n",
    "l2 = loss(Y_pred_bad, Y)\n",
    "\n",
    "print(l1.item())  #it has only one value so we can call item\n",
    "print(l2.item()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "08aeabaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0])\n",
      "tensor([1])\n"
     ]
    }
   ],
   "source": [
    "_, predictions1 = torch.max(Y_pred_good, 1)\n",
    "_, predictions2 = torch.max(Y_pred_bad, 1)\n",
    "\n",
    "print(predictions1) #the highest element in the Y_pred\n",
    "print(predictions2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "48133fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3018244206905365\n",
      "2.2682371139526367\n",
      "tensor([2, 0, 1])\n",
      "tensor([1, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "#the loss in pytorch allows for multiple samples\n",
    "#so let's say we have 3 samples \n",
    "Y = torch.tensor([2, 0, 1])\n",
    "\n",
    "#n_samples x n_classes = 3 x 3 \n",
    "Y_pred_good = torch.tensor([[0.1, 1.0, 2.1],[2.0, 1.0, 0.1],[0.1, 3.0, 0.1]])\n",
    "Y_pred_bad = torch.tensor([[0.5, 2.0, 0.3],[0.0, 1.0, 0.1],[0.1, 0.0, 3.1]]) \n",
    "\n",
    "#compute the loss\n",
    "l1 = loss(Y_pred_good, Y)\n",
    "l2 = loss(Y_pred_bad, Y)\n",
    "\n",
    "print(l1.item())  #it has only one value so we can call item\n",
    "print(l2.item()) \n",
    "\n",
    "_, predictions1 = torch.max(Y_pred_good, 1)\n",
    "_, predictions2 = torch.max(Y_pred_bad, 1)\n",
    "\n",
    "print(predictions1) #the highest element in the Y_pred\n",
    "print(predictions2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9c77ba",
   "metadata": {},
   "source": [
    "## Example of a multiclass problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4e9fd93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet2(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet2, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)  #one linear layer\n",
    "        self.relu = nn.ReLU()                              #activation function\n",
    "        self.linear2 = nn.Linear(hidden_size,num_classes)  #last layer\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.linear1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        #as we are using nn.CrossEntropy no softmax will be here.\n",
    "        return out\n",
    "\n",
    "model = NeuralNet2(input_size=28*28, hidden_size=5, num_classes=3)\n",
    "criterion = nn.CrossEntropyLoss() #applies softmax too"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0523fd",
   "metadata": {},
   "source": [
    "## Example of a binary classification problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ede236b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet1(nn.Module):\n",
    "    #num_classes = 1\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(NeuralNet1, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)  #one linear layer\n",
    "        self.relu = nn.ReLU()                              #activation function\n",
    "        self.linear2 = nn.Linear(hidden_size, 1)           #last layer\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.linear1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        #sigmoid at the end\n",
    "        y_pred = torch.sigmoid(out)\n",
    "        return y_pred\n",
    "\n",
    "model = NeuralNet1(input_size=28*28, hidden_size=5)\n",
    "criterion = nn.BCELoss() #applies softmax too"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
