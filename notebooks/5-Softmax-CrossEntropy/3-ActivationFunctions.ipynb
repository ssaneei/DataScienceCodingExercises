{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc794b85",
   "metadata": {},
   "source": [
    "## Activation Functions\n",
    "(based on a tutorial by Python Engineer in Youtube)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcf1776",
   "metadata": {},
   "source": [
    "**Activation Function**\n",
    " \n",
    " Without activation functions the whole network would be equal to a linear function which is not able to solve more complicated tasks.\n",
    " \n",
    " So after each layer, we add an activation function so that our network can learn better.\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879c6835",
   "metadata": {},
   "source": [
    "**Different Types of Activation Functions**\n",
    "\n",
    "\n",
    "    1- Step function\n",
    "                will have 1 as output if the input is greater than threshold. (not used in practice)\n",
    "    2- Sigmoid\n",
    "                formula,\n",
    "                output between 0 and 1,\n",
    "                in the last layer of binary classification problem.\n",
    "    3- TanH\n",
    "                formula\n",
    "                scaled sigmoid and also a bit shifted\n",
    "                output between -1 and 1\n",
    "                good choice in the hidden layers\n",
    "    4- ReLU\n",
    "                most popular choice in most of the networks\n",
    "                output 0 for negative values and output the same input for positive inputs =>\n",
    "                a linear function to values greater than 0 and 0 for negative values. => non linear\n",
    "                ***if you don't know what function you should use, use the ReLU for hidden layers.\n",
    "    5- Leaky ReLU\n",
    "                modified/improved version of ReLU\n",
    "                for negative inputs there would be a multiplication of input with a very small number\n",
    "                (a = 0.001)\n",
    "                \n",
    "                Tries to solve vanishing gradient problem. (with the normal ReLU the values for negative part is \n",
    "                zero that means the derivation/gradient in backpropagation would be zero, too. This means that \n",
    "                those weights will never be updated => Those neurons won't learn anything.\n",
    "                (the neurons are dead)\n",
    "                \n",
    "                So whenever you notice that the weights won't get updated during the training, use the \n",
    "                LeakyReLU.\n",
    "                \n",
    "    6- Softmax\n",
    "                Squash the input to be outputs between zero and 1 and sum to one, so the probabilities.\n",
    "                \n",
    "                => a good choice for the last layer of a multiclass classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e036a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9c77ba",
   "metadata": {},
   "source": [
    "## Two ways to use the activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5c8b5c",
   "metadata": {},
   "source": [
    "## 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e9fd93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet2(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet2, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)  #one linear layer\n",
    "        self.relu = nn.ReLU()                              #activation function\n",
    "        self.linear2 = nn.Linear(hidden_size,num_classes)  #last layer\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.linear1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        #as we are using nn.CrossEntropy no softmax will be here.\n",
    "        return out\n",
    "\n",
    "model = NeuralNet2(input_size=28*28, hidden_size=5, num_classes=3)\n",
    "criterion = nn.CrossEntropyLoss() #applies softmax too"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0523fd",
   "metadata": {},
   "source": [
    "## 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ede236b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet3(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet3, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)  #one linear layer\n",
    "        self.linear2 = nn.Linear(hidden_size, num_classes) #last layer\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = torch.relu(self.linear1(x))\n",
    "        out = torch.sigmoid(self.linear2(out))\n",
    "        #sigmoid at the end\n",
    "        return y_pred\n",
    "\n",
    "model = NeuralNet3(input_size=28*28, hidden_size=5, num_classes=3)\n",
    "criterion = nn.CrossEntropyLoss() #applies softmax too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c057fcf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
