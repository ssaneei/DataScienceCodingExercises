{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55300acd",
   "metadata": {},
   "source": [
    "## Feed Forward Neural Network\n",
    "\n",
    "(based on a tutorial by Python Engineer in Youtube)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533fd19c",
   "metadata": {},
   "source": [
    "Here we will implement a multilayer neural network that can do digit classification based on MNIST dataset.\n",
    "\n",
    "What we will use:\n",
    "\n",
    "    - DataLoader: to load the dataset\n",
    "    - Transformation: apply a transform to dataset\n",
    "    - Multilayer Neural Net: input,hidden,output layers\n",
    "    - Activation Function\n",
    "    - Loss\n",
    "    - Optimizer\n",
    "    - Training Loop (batch training)\n",
    "    - Model Evaluation\n",
    "    - GPU support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edaa7a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "622b1cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf /logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b62818cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#device config\n",
    "#if we have GPU it will work with it else with CPU, we must push our tensors to the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ab2dcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "input_size = 784   #because the images are 28x28 and we will flat the array to 1D tensor\n",
    "hidden_size = 100  #you can use other dim too.\n",
    "num_classes = 10   #we have 10 classes in the dataset\n",
    "num_epochs = 2      #so that it won't get long time\n",
    "batch_size = 100\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d4e36889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import MNIST data\n",
    "#must be in the same folder, it's for training, we add a transform and convert data to tensors, it must be \n",
    "#downloaded if we have'nt done that yet\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True\n",
    "                                          ,transform = transforms.ToTensor(), download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=False\n",
    "                                          ,transform = transforms.ToTensor())\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size\n",
    "                                          , shuffle=True) # shuffle is good for training\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d86be8c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "#let's look at one batch of this data\n",
    "examples = iter(train_loader)\n",
    "samples, labels = examples.next()\n",
    "print(samples.shape, labels.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133aa32d",
   "metadata": {},
   "source": [
    "samples:\n",
    "100 is the batch size.\n",
    "1 is the number of color channels.\n",
    "28x28 the image array.\n",
    "\n",
    "labels:\n",
    "For each class label we have one value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "42e5a359",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot\n",
    "for i in range(6):\n",
    "    plt.subplot(2, 3, i+1)   #2 raws, 3 columns, in index i+1\n",
    "    plt.imshow(samples[i][0], cmap='gray')\n",
    "    #we want to show the actual data here, [i][0] to access the first channel, and the color map is set to gray\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db8d20e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we want to classify these digits so we need a fully connected neural network with one hidden layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c47d9256",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes): #num_classes is the output size\n",
    "        super(NeuralNet, self).__init__()\n",
    "        \n",
    "        #fist layer\n",
    "        self.l1 = nn.Linear(input_size, hidden_size)\n",
    "        \n",
    "        #activation function\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        #another linear layer\n",
    "        self.l2 = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #now we use all above layers\n",
    "        out = self.l1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.l2(out)\n",
    "        \n",
    "        #cross entropy will implement softmax,too, so we don't add softmax here\n",
    "        \n",
    "        return out\n",
    "    \n",
    "\n",
    "model = NeuralNet(input_size, hidden_size, num_classes)\n",
    "\n",
    "#loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1c6b8c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/ 2, step 100/600,loss = 0.3966\n",
      "epoch 1/ 2, step 200/600,loss = 0.4475\n",
      "epoch 1/ 2, step 300/600,loss = 0.2905\n",
      "epoch 1/ 2, step 400/600,loss = 0.2520\n",
      "epoch 1/ 2, step 500/600,loss = 0.1490\n",
      "epoch 1/ 2, step 600/600,loss = 0.2753\n",
      "epoch 2/ 2, step 100/600,loss = 0.2251\n",
      "epoch 2/ 2, step 200/600,loss = 0.2763\n",
      "epoch 2/ 2, step 300/600,loss = 0.1613\n",
      "epoch 2/ 2, step 400/600,loss = 0.1172\n",
      "epoch 2/ 2, step 500/600,loss = 0.3433\n",
      "epoch 2/ 2, step 600/600,loss = 0.1171\n"
     ]
    }
   ],
   "source": [
    "#training loop\n",
    "n_total_steps = len(train_loader)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):    #unpacking, enumerate give us the index\n",
    "        #we have to reshape our images first because this is 100 * 1 * 28 * 28\n",
    "        #but now input size is 784 so our images tensors needs the size 100 * 784\n",
    "        #first the number of batches then images size\n",
    "        images = images.reshape(-1, 28*28).to(device) # and send it to GPU if it's there\n",
    "        \n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        #forward\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels) #predicted output and actual labels\n",
    "        \n",
    "        \n",
    "        #backward\n",
    "        \n",
    "        #empty the grads\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        #update\n",
    "        optimizer.step()\n",
    "        \n",
    "        if(i+1)%100 == 0:\n",
    "            print(f'epoch {epoch+1}/ {num_epochs}, step {i+1}/{n_total_steps},loss = {loss.item():.4f}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cce4682c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "accuracy = 98.0\n",
      "200\n",
      "accuracy = 98.5\n",
      "300\n",
      "accuracy = 97.66666666666667\n",
      "400\n",
      "accuracy = 96.75\n",
      "500\n",
      "accuracy = 96.4\n",
      "600\n",
      "accuracy = 95.83333333333333\n",
      "700\n",
      "accuracy = 95.42857142857143\n",
      "800\n",
      "accuracy = 95.375\n",
      "900\n",
      "accuracy = 95.22222222222223\n",
      "1000\n",
      "accuracy = 95.2\n",
      "1100\n",
      "accuracy = 94.81818181818181\n",
      "1200\n",
      "accuracy = 94.58333333333333\n",
      "1300\n",
      "accuracy = 94.23076923076923\n",
      "1400\n",
      "accuracy = 94.14285714285714\n",
      "1500\n",
      "accuracy = 94.2\n",
      "1600\n",
      "accuracy = 94.0\n",
      "1700\n",
      "accuracy = 94.11764705882354\n",
      "1800\n",
      "accuracy = 94.11111111111111\n",
      "1900\n",
      "accuracy = 94.10526315789474\n",
      "2000\n",
      "accuracy = 94.05\n",
      "2100\n",
      "accuracy = 93.76190476190476\n",
      "2200\n",
      "accuracy = 93.54545454545455\n",
      "2300\n",
      "accuracy = 93.52173913043478\n",
      "2400\n",
      "accuracy = 93.5\n",
      "2500\n",
      "accuracy = 93.56\n",
      "2600\n",
      "accuracy = 93.6923076923077\n",
      "2700\n",
      "accuracy = 93.74074074074075\n",
      "2800\n",
      "accuracy = 93.82142857142857\n",
      "2900\n",
      "accuracy = 93.96551724137932\n",
      "3000\n",
      "accuracy = 93.96666666666667\n",
      "3100\n",
      "accuracy = 94.03225806451613\n",
      "3200\n",
      "accuracy = 94.0\n",
      "3300\n",
      "accuracy = 94.0909090909091\n",
      "3400\n",
      "accuracy = 94.20588235294117\n",
      "3500\n",
      "accuracy = 94.25714285714285\n",
      "3600\n",
      "accuracy = 94.13888888888889\n",
      "3700\n",
      "accuracy = 94.21621621621621\n",
      "3800\n",
      "accuracy = 94.13157894736842\n",
      "3900\n",
      "accuracy = 94.02564102564102\n",
      "4000\n",
      "accuracy = 93.925\n",
      "4100\n",
      "accuracy = 93.90243902439025\n",
      "4200\n",
      "accuracy = 93.9047619047619\n",
      "4300\n",
      "accuracy = 93.88372093023256\n",
      "4400\n",
      "accuracy = 93.9090909090909\n",
      "4500\n",
      "accuracy = 93.91111111111111\n",
      "4600\n",
      "accuracy = 93.8695652173913\n",
      "4700\n",
      "accuracy = 93.8936170212766\n",
      "4800\n",
      "accuracy = 93.91666666666667\n",
      "4900\n",
      "accuracy = 93.85714285714286\n",
      "5000\n",
      "accuracy = 93.9\n",
      "5100\n",
      "accuracy = 94.0\n",
      "5200\n",
      "accuracy = 94.07692307692308\n",
      "5300\n",
      "accuracy = 94.15094339622641\n",
      "5400\n",
      "accuracy = 94.24074074074075\n",
      "5500\n",
      "accuracy = 94.30909090909091\n",
      "5600\n",
      "accuracy = 94.41071428571429\n",
      "5700\n",
      "accuracy = 94.36842105263158\n",
      "5800\n",
      "accuracy = 94.41379310344827\n",
      "5900\n",
      "accuracy = 94.42372881355932\n",
      "6000\n",
      "accuracy = 94.41666666666667\n",
      "6100\n",
      "accuracy = 94.36065573770492\n",
      "6200\n",
      "accuracy = 94.38709677419355\n",
      "6300\n",
      "accuracy = 94.47619047619048\n",
      "6400\n",
      "accuracy = 94.5\n",
      "6500\n",
      "accuracy = 94.56923076923077\n",
      "6600\n",
      "accuracy = 94.54545454545455\n",
      "6700\n",
      "accuracy = 94.58208955223881\n",
      "6800\n",
      "accuracy = 94.58823529411765\n",
      "6900\n",
      "accuracy = 94.65217391304348\n",
      "7000\n",
      "accuracy = 94.72857142857143\n",
      "7100\n",
      "accuracy = 94.80281690140845\n",
      "7200\n",
      "accuracy = 94.86111111111111\n",
      "7300\n",
      "accuracy = 94.9041095890411\n",
      "7400\n",
      "accuracy = 94.95945945945945\n",
      "7500\n",
      "accuracy = 94.96\n",
      "7600\n",
      "accuracy = 95.0\n",
      "7700\n",
      "accuracy = 95.06493506493507\n",
      "7800\n",
      "accuracy = 95.12820512820512\n",
      "7900\n",
      "accuracy = 95.12658227848101\n",
      "8000\n",
      "accuracy = 95.1625\n",
      "8100\n",
      "accuracy = 95.1604938271605\n",
      "8200\n",
      "accuracy = 95.1951219512195\n",
      "8300\n",
      "accuracy = 95.20481927710843\n",
      "8400\n",
      "accuracy = 95.25\n",
      "8500\n",
      "accuracy = 95.27058823529411\n",
      "8600\n",
      "accuracy = 95.30232558139535\n",
      "8700\n",
      "accuracy = 95.35632183908046\n",
      "8800\n",
      "accuracy = 95.4090909090909\n",
      "8900\n",
      "accuracy = 95.46067415730337\n",
      "9000\n",
      "accuracy = 95.5111111111111\n",
      "9100\n",
      "accuracy = 95.48351648351648\n",
      "9200\n",
      "accuracy = 95.53260869565217\n",
      "9300\n",
      "accuracy = 95.56989247311827\n",
      "9400\n",
      "accuracy = 95.6063829787234\n",
      "9500\n",
      "accuracy = 95.6421052631579\n",
      "9600\n",
      "accuracy = 95.67708333333333\n",
      "9700\n",
      "accuracy = 95.65979381443299\n",
      "9800\n",
      "accuracy = 95.56122448979592\n",
      "9900\n",
      "accuracy = 95.54545454545455\n",
      "10000\n",
      "accuracy = 95.53\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "with torch.no_grad():\n",
    "    n_correct = 0 \n",
    "    n_samples = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, 28*28).to(device) # and send it to GPU if it's there\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        \n",
    "        #torch.max returns value and index but we need index\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        n_samples += labels.shape[0]  #number of sample and current batch = should be 100\n",
    "        print(n_samples)\n",
    "        n_correct += (predictions == labels).sum().item()\n",
    "        \n",
    "        \n",
    "    acc = 100.0 * n_correct / n_samples #accuracy\n",
    "    print(f'accuracy = {acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eff28c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04ec5cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
