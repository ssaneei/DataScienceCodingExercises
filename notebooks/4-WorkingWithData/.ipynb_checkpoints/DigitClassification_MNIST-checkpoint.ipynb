{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55300acd",
   "metadata": {},
   "source": [
    "## Feed Forward Neural Network\n",
    "\n",
    "(based on a tutorial by Python Engineer in Youtube)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533fd19c",
   "metadata": {},
   "source": [
    "Here we will implement a multilayer neural network that can do digit classification based on MNIST dataset.\n",
    "\n",
    "What we will use:\n",
    "\n",
    "    - DataLoader: to load the dataset\n",
    "    - Transformation: apply a transform to dataset\n",
    "    - Multilayer Neural Net: input,hidden,output layers\n",
    "    - Activation Function\n",
    "    - Loss\n",
    "    - Optimizer\n",
    "    - Training Loop (batch training)\n",
    "    - Model Evaluation\n",
    "    - GPU support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edaa7a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b62818cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#device config\n",
    "#if we have GPU it will work with it else with CPU, we must push our tensors to the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ab2dcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "input_size = 784   #because the images are 28x28 and we will flat the array to 1D tensor\n",
    "hidden_size = 100  #you can use other dim too.\n",
    "num_classes = 10   #we have 10 classes in the dataset\n",
    "num_epochs = 2      #so that it won't get long time\n",
    "batch_size = 100\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d4e36889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import MNIST data\n",
    "#must be in the same folder, it's for training, we add a transform and convert data to tensors, it must be \n",
    "#downloaded if we have'nt done that yet\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True\n",
    "                                          ,transform = transforms.ToTensor(), download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=False\n",
    "                                          ,transform = transforms.ToTensor())\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size\n",
    "                                          , shuffle=True) # shuffle is good for training\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d86be8c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "#let's look at one batch of this data\n",
    "examples = iter(train_loader)\n",
    "samples, labels = examples.next()\n",
    "print(samples.shape, labels.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133aa32d",
   "metadata": {},
   "source": [
    "samples:\n",
    "100 is the batch size.\n",
    "1 is the number of color channels.\n",
    "28x28 the image array.\n",
    "\n",
    "labels:\n",
    "For each class label we have one value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "42e5a359",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD6CAYAAAC4RRw1AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAdzUlEQVR4nO3de5AUxR0H8O9P5KGACirkJCdgBA2g+ECCwfiIuRRSKcCURBAVKCIaJREVIogmpAwRDQWxEEUq8tRA5BUoFBEOxWiECAQBBQJoxBPCiSggpQih88ctTfdwuzu3OzszPfv9VF3dr7d3dxp+XDP32+4ZUUqBiIjcc1LUAyAiotxwAicichQncCIiR3ECJyJyFCdwIiJHcQInInJUXhO4iHQRkS0isk1EhgU1KIoW85pczG2ySK7rwEWkFoB/AygDUAHgHQC9lVLvBzc8ChvzmlzMbfKcnMdrOwLYppT6AABEZBaA7gDS/mMQEe4aigmllKTpYl7dtkcpdXaavhrllnmNlWrzmk8JpRmAj412Reoxi4gMFJHVIrI6j2NReJhXt32UoS9rbpnX2Ko2r/mcgVd3BnfC/9hKqUkAJgH8H90RzGtyZc0t8+qWfM7AKwCUGu1vA9iZ33AoBpjX5GJuEyafCfwdAK1EpKWI1AHQC8DCYIZFEWJek4u5TZicSyhKqSMiMgjAEgC1AExWSr0X2MgoEsxrcjG3yZPzMsKcDsaamrZhwwarXVFRoeO+fftafZWVlYEfP8MqlBpjXmNljVKqQxBvxLzGSrV55U5MIiJHcQInInIUJ3AiIkflsw6camjq1Kk6btu2rdXXpk0bHZeUlFh9haiBJ0mnTp103K9fP6vvzjvv1PH8+fOtvuuvv17H3s+Chg4darXNzygWL16c81iJgsQzcCIiR3ECJyJyFJcRBqx+/fo6HjdunNVn/np/8sl29eq9944vxzVLAgBw8ODBAEdYxeVlhPXq1bPaEydO1PFtt91WkGPu379fx+vWrbP6+vTpo+OdOyPf2Fj0ywhLS49vNr3jjjusvmuuucZqr1+/Xsdr1qyx+ubNm6djM/8R4TJCIqIk4QROROQoTuBERI5iDTxg1113nY6XLVuW9nkidgl6wIABOp4yZUrwA/NwuQZ+wQUXWO33309/Q5kJEyboeMuWLWmfd9ppp1nt3//+977H869//UvHXbt2tfoiWALqbA185MiRVvvzzz/X8ZNPPmn1mXXuQYMGWX3mpSjOOussq8/7c5dp/jN/fh966CGrb+3atWlfVyCsgRMRJQkncCIiR7GEkqchQ4ZYbbMU0rp167SvGzVqlNV+/PHHdVyIZYNeLpdQvL8G33LLLTpu3Lix1WeWUI4ePer7PevUqWO1p02bpuOePXumfZ/+/ftb7enTp6d9boE4VUIxl92uXLnS6mvRooWOJ0+ebPXdfvvtOm7YsKHVt2nTJh3Pnj3b6stUQjF/dgGgWbPjd5tbvdq+w5xZKv3qq68QApZQiIiShBM4EZGjOIETETmKNfAa+sUvfmG1vcuLzLqZt5Zt1rn/9re/WX0bN24MaIT+uFwDj0Lnzp11/NJLL1l9Zg123759Vl+vXr10/OqrrxZodBanauC33nqrjs2rdVYzFqttzlvl5eVWn1nLNq8imU3dunWt9vjx43Vs5hEAli9fruMePXr4PkYeWAMnIkoSTuBERI7iDR18MJcKPvHEE1aftwT19ddf69h7c4G5c+cGPzgKxVtvvaXjgQMHWn0zZ87U8emnn271NWnSpLADc1z37t19Pc/clQnYJY3XX3/d6jty5EhOYzl06JDVNvO8Y8cOq2/EiBE69i4dDWMn9TE8AycichQncCIiR3ECJyJyFJcRpuR6Jx2z5g3Y27q9SwXjhMsIc3fGGWdY7c8++yztc80r4z3//POFGpLJqWWEu3fv1rH3yoFm3fvaa6+1+sJeduu1YsUKHXtvQn7ZZZfp+MsvvwzqkFxGSESUJFkncBGZLCKVIrLReKyxiCwVka2p740KO0wKGvOaXMxt8fCzjHAqgKcAmJdVGwagXCk1WkSGpdoPBj+8wvEu77rzzjt17L0qWSbmDXWBeJdNPKYigXklAA7l1izhzpkzx+qbNWuWjqMumXiZN/xYvHix1WcuMRw+fHhBx5H1DFwp9QaAvZ6HuwM4dn3NaQB6BDssKjTmNbmY2+KRaw28qVJqFwCkvnO3QjIwr8nF3CZQwXdiishAAAOzPpGcwrwmE/Pqllwn8N0iUqKU2iUiJQDS3rlVKTUJwCQgXsvN7rnnHqv98MMP+3pd7969rfaiRYsCG1MMOJ9XSstXbgud127dulntM888U8feu+fMnz8/6MMH5t1339WxeQcgwJ5bZsyYYfVlugF3LnItoSwEcGyBa18AC4IZDkWMeU0u5jaB/CwjnAngbQAXiEiFiAwAMBpAmYhsBVCWapNDmNfkYm6LR9YSilKqd5qu6wMeS8GZO6ZuuOEGq897wXiTuTTwr3/9a+DjikKS8hq29u3bRz2EjOKcW+9NEzL93MVZZeXxCtQvf/lLq89cVtimTRurLy4lFCIiihgncCIiR3ECJyJyVFHdkce8+ejll19u9ZlbeleuXGn1/epXv/J9jHr16un4rrvusvq++uorHT/77LO+35OiZ9Zu77///ghHQnHjvSOQ98bWhcQzcCIiR3ECJyJyVKJLKGPHjrXa3pKGyfw1yLssqKKiIu3rzj//fKtt3gyia9euaV/nvTHEhAkT0j6XonfNNdfo+Cc/+Una53lvfrthw4aCjclFP/jBD6y2q8sIMwnzz8QzcCIiR3ECJyJyFCdwIiJHJa4G3qjR8TtFXXTRRVafucTP3AoLAL/+9a91XJPtrpdcconV/tGPfuTrdd6rsrEGnrsLLrhAx82bNy/IMbzLTtN5+eWXrbZ51ToC2rVrZ7XDvKl6oXhvuHzaaaeFdmyegRMROYoTOBGRoziBExE5KnE18FtvvVXHP/zhD9M+z7v9dc2aNb7e37vue8qUKVa7Tp06vt7n1FNP9fW8YnXKKadYbXMddv/+/a2+iy++WMetW7cu7MCymDRpUqTHj7upU6da7auvvjqageTJ3Mfh/Tyrdu3aoY2DZ+BERI7iBE5E5CjnSyjeX7X79eunY++W1t27d+v4gQceyOl4jz76qNWuX7++1TaPmWmJ1CeffJLT8ZOsSZMmOp47d67V9/3vfz/s4eTkT3/6k9W+7rrrohlITJlX5HSZmVfv1UoPHz6s408//bSg4+AZOBGRoziBExE5ihM4EZGjnK+Bn3feeVb70KFDOvbWoFetWqXjmtSghwwZouObb77Z6vMew+/WYHNbf7EqKSmx2n/5y1907K15m59fbNmyxeqbM2eOjpcuXZr2eC1atLDaL774oo4bNmyYfcA+dOjQwWqbl5O98cYbrb5t27YFckyXLFmyxGr//e9/17H3TkfLly/X8d69ews7sBqaOXNm2r4333xTxytWrCjoOHgGTkTkKE7gRESOcr6E8tFHH1ntTDcUNa9O2KxZM6vPLKl4d1N+73vfy2eI1eJOTGDevHlWu2PHjjo2l2IBQK9evXT8xhtvpH3PWrVqWW3zKpPmewB22cRb+jJLcQAwfPhwHXt/nS8rK9PxTTfdZPWZV0pcuHCh1Wfe2eeDDz5AMdi/f7/VNksMjzzyiNU3fvx4Hffp06ewA4O9jBUA2rdvr2NvyeTMM8/U8RdffGH1eZcaFxLPwImIHMUJnIjIUVkncBEpFZHXRGSTiLwnIvemHm8sIktFZGvqe6Ns70XxwbwmVm3mtXhItmVvIlICoEQptVZEGgJYA6AHgH4A9iqlRovIMACNlFIPZnmvwG+/4V2OV15eruNOnTpZfQcOHNDx008/bfWZV5Hr3Lmz1Td9+nQde7fn53pHEW+tNgLnIOK8Hj161Gqbf5fm0kAAuOWWW3Tco0cPq++cc87RsVm3BE68cmE65hJGALjtttt8vS4b89+Ot45rfn4zbtw4q8+s/9bQegD94/rz6mXe0Wr16tVpnzdq1Cir/dRTT+m4JtvVL7vsMqt933336di7dDXT3Z3MuaRnz55W37Jly3yPpwbWKKU6eB/MegaulNqllFqbig8A2ASgGYDuAKalnjYNVf9IyBHMa2IdZl6LR41WoYhICwCXAlgFoKlSahdQNRmISJM0rxkIYGCe46QCYl6TiXlNvqwlFP1EkQYAVgAYpZSaJyJfKKXOMPo/V0plrKuF8SuZudvN+2t4pisF7tmzR8d169a1+szlZjUpoezcudNqm78GTpw4Me3rwqCUEiDavHqXzmX6lXXjxo06btmypdXnvSKkafPmzTqeMWOG1bdo0SId79ixw+rzLnfLVWlpqY5PP/10368z/7w1tEYp1cGVn9eTTjpeBDB3PAP2cjzzBgqAvXTPexONxo0b67h79+5Wn3fHrfdn3WQuZTV3VwLAz372Mx1//vnnad8jQLmVUABARGoDmAvgBaXUscW7u1P18WN18sp0r6d4Yl6TiXktHn5WoQiA5wBsUkqNNboWAuibivsCWBD88KhQmNdEY16LhJ8aeGcAtwHYICLrUo89BGA0gBdFZACAHQB6Vv9yiinmNZkagHktGr5r4IEcLISaGvlzrAYehFzzeuGFF1ptc6u7d1t1JrNnz9ax92qEr7zyio6L5C5I1dZKcxH1z+uPf/xjHS9evNjqy3Xe8n6GtW7dOh17P7MaM2aMjgt9VUEfcq+BExFR/HACJyJyFEsoRSoOJRQqiMSUUExXXXWV1W7btq2OvbttzZ25CxbYn9U+88wzVtvcDXvw4MG8x1lALKEQESUJJ3AiIkdxAicichRr4EWKNfDESmQNnFgDJyJKFE7gRESO4gROROQoTuBERI7iBE5E5ChO4EREjuIETkTkKE7gRESO4gROROQoTuBERI7iBE5E5ChO4EREjuIETkTkKD93pQ/SHgAfATgrFcdBMY6lecDvx7xmFuZYgswt85pZ5HkN9XKy+qAiq4O65GW+OJbgxGn8HEtw4jR+jsXGEgoRkaM4gRMROSqqCXxSRMetDscSnDiNn2MJTpzGz7EYIqmBExFR/lhCISJyFCdwIiJHhTqBi0gXEdkiIttEZFiYx04df7KIVIrIRuOxxiKyVES2pr43CmEcpSLymohsEpH3ROTeqMYSBObVGkticsu8WmOJZV5Dm8BFpBaACQBuANAGQG8RaRPW8VOmAujieWwYgHKlVCsA5al2oR0B8IBS6rsAOgG4J/V3EcVY8sK8niARuWVeTxDPvCqlQvkCcCWAJUZ7OIDhYR3fOG4LABuN9hYAJam4BMCWCMa0AEBZHMbCvDK3zKs7eQ2zhNIMwMdGuyL1WNSaKqV2AUDqe5MwDy4iLQBcCmBV1GPJEfOahuO5ZV7TiFNew5zApZrHinoNo4g0ADAXwGCl1P6ox5Mj5rUaCcgt81qNuOU1zAm8AkCp0f42gJ0hHj+d3SJSAgCp75VhHFREaqPqH8ILSql5UY4lT8yrR0Jyy7x6xDGvYU7g7wBoJSItRaQOgF4AFoZ4/HQWAuibivuiqrZVUCIiAJ4DsEkpNTbKsQSAeTUkKLfMqyG2eQ258N8VwL8BbAcwIoIPHmYC2AXgMKrOMAYAOBNVnx5vTX1vHMI4rkLVr6PrAaxLfXWNYizMK3PLvLqbV26lJyJyFHdiEhE5ihM4EZGj8prAo95qS4XBvCYXc5sweRT1a6Hqw43zANQB8C6ANlleo/gVjy/mNbFfnwaV2xj8WfiVJa/5nIF3BLBNKfWBUuobALMAdM/j/SgemFe3fZShj7l1V7V5zWcC97XVVkQGishqEVmdx7EoPMxrcmXNLfPqlpPzeK2vrbZKqUlI3XpIRE7op9hhXpMra26ZV7fkcwYe1622lB/mNbmY24TJZwKP61Zbyg/zmlzMbcLkXEJRSh0RkUEAlqDq0+3JSqn3AhsZRYJ5TS7mNnlC3UrPmlp8KKWqq4fmhHmNlTVKqQ5BvBHzGivV5pU7MYmIHMUJnIjIUZzAiYgcxQmciMhRnMCJiBzFCZyIyFH5bKUnSoz77rvPao8dO9Zq+11uO2DAAKs9ZcqU/AZGWT3yyCNWe/369TpesCDutx7ND8/AiYgcxQmciMhRnMCJiBzFGjgVraFDh+r40UcftfqOHj2a03s+/fTTVrtOnTo6fvbZZ3N6T8qsc+fOVrtVq1Y6Zg2ciIhiiRM4EZGjWELJ029+8xur3adPHx23bt3a6quoqNBxWVmZ1bd58+YCjI4yMZeb7du3z+o766yzcnpPs2QCAGPGjNGxN8crVqzI6RiUWc+ePXV8++23RziSwuMZOBGRoziBExE5ihM4EZGjWAP3oV69ejr2bo3u1q2b1T7llFN07F2Kds455+i4d+/eVt9vf/vbvMdJNbNy5Uodf/jhh1Zfphr4J598YrW/+eYbHbds2dLqO/XUU3Xcpk0bq4818GB89tlnVtv8HOKSSy6x+tatWxfCiMLDM3AiIkdxAicichRLKNWoX7++1X777bd13LZt24yv9XvVujBvJk1VGjZsaLXNnZFXXHFFxtfu2bNHx94rFS5cuFDHb731ltXXpEkTHd99991W3zPPPJNlxOTHsmXLrHavXr10fOGFF1p9LKEQEVEscAInInIUJ3AiIkexBp5St25dHc+YMcPqa9eunY6z1a5FxNdzzaWJVDjmkrKpU6dafT169Ej7Om/uzEsmZLqq4Isvvmi1Bw0a5GOUlI/FixdHPYTI8AyciMhRWSdwEZksIpUistF4rLGILBWRranvjQo7TAoa85pczG3xEB8lgasBfAlgulKqXeqxJwDsVUqNFpFhABoppR7MejCR2KydM3dMAvaNUR980P6j+C2L1OS5O3futNqlpaUZ37cArkEC8+p177336ti7/C+T5557zmoPHDjQ1+u2bdtmtc2dme+//77Vd9FFF/keTw2sAXA/AshtnPNq8pYjzZ2Zs2bNsvq8N512yBqlVAfvg1nPwJVSbwDY63m4O4BpqXgagB75jo7CxbwmF3NbPHL9ELOpUmoXACildolIk3RPFJGBAPydvlDUmNfk8pVb5tUtBV+FopSaBGAS4M6vZJQd85pMzKtbcp3Ad4tISep/8hIAlUEOKgzmFmfgxLq3qSbb3v0+d+3atb7fM0TO57VTp05We/To0b5e9+6771rtu+66K7AxHTNixIjA37MGnM9tOl9//bXVfvXVV3V8+eWXW30nn2xPeUeOHCncwEKQ6zLChQD6puK+AJJ96+fiwbwmF3ObQH6WEc4E8DaAC0SkQkQGABgNoExEtgIoS7XJIcxrcjG3xSNrCUUp1TtN1/UBjyVUBw8etNq7d+/WcdOmTa2+QiwjnD9/vq9xFkqS8nr11Vfr2LwyIHDiTYZNL7/8so4fe+wxq897M45Mzj//fB03aNDA6qusPF6p2LFjh+/3zEeScpuL//3vfzq++OKLrT7vjca9Sztdw52YRESO4gROROQoTuBERI4q2qsRmndYAYBx48bp2Luk8MCBAzr23oy4S5cuVtvcku+1fPlyHU+fPt3/YMli3igYAIYOHapj7113TP/973+t9uDBg3W8fft238c/77zzrLZ5Nbyzzz7b6nvjjTd0nLS7wcTVnDlzdPzTn/7U6uvYsaPVZg2ciIgiwQmciMhRRVtC8frjH/+o49mzZ1t9e/cevy5Q7dq1rb6ZM2dabXMZ4TfffGP1PfnkkzquyTI1snnLVF27dk373E8//VTHPXv2tPpqUjZp0aKFjpcsWWL1eUsqpj//+c++j0HBM38ek4hn4EREjuIETkTkKE7gRESOYg28Gv/5z3/S9o0cOdJqn3vuuVbb3D6/dOlSq2/RokV5j61YmVvivdujMzE/o/jHP/7h+3W1atWy2g8//LCOM9W8//nPf1rtV155xfcxKXg1uZKoi3gGTkTkKE7gRESO4gROROQo1sBraPjw4Rn7zZrbvHnzCj2conHFFVfo2Hv5ApN5KVHgxMvLpnPSSfa5zN133221+/fvn/a15nr/CRMmWH3mHdIpHKtXr07b571Dz9SpUws8msLiGTgRkaM4gRMROYolFB9uuukmHXuXl3mZZZMpU6YUbExJ571zyowZM3y97ne/+53Vfu2113y97v7777fajz/+uK/XAfYNss0rV1I0tm3blrbPvHtSEvAMnIjIUZzAiYgcxQmciMhRrIFXw7tV2qxlZ7s8ZdR3m08Kbw28efPmaZ9r1p3HjBnj+xilpaU6zrRMELCXJ3rvyvTll1/6PiZF64svvoh6CIHiGTgRkaM4gRMROYollGr8/Oc/t9rem+iavHdn8d6hhwrP3P166NChtM8bMWKE1TZ3W37rW9/KeIxRo0bpeO7cuTUdIsWE9+fVdTwDJyJyVNYJXERKReQ1EdkkIu+JyL2pxxuLyFIR2Zr63qjww6WgMK+JVZt5LR5+zsCPAHhAKfVdAJ0A3CMibQAMA1CulGoFoDzVJncwr8nFvBaJrDVwpdQuALtS8QER2QSgGYDuAK5NPW0agNcBPFiQUYagQ4cOOh48eLDVl2np4B/+8IdCDamg4p7XQYMG+X5u/fr1dfz222+nfd6ll15qtWvXrp32uZs3b7baGzdu9D2eiB1WSq0F4pnXMHTv3j3qIYSmRh9iikgLAJcCWAWgaWoSgFJql4g0SfOagQAG5jlOKiDmNZmY1+TzPYGLSAMAcwEMVkrtz7ah5Ril1CQAk1Lvkewb1DmIeU0m5rU4+JrARaQ2qv4xvKCUOna5vd0iUpL637wEQGWhBlkI5hXkAOCll17Scd26da0+c5na9u3brb4NGzYUYHThiHNey8vLrXZZWVna55pXiOzYsaPvYxw8eFDHK1eutPr69etntXfu3On7faMW57yGwZs7U6YrFbrIzyoUAfAcgE1KqbFG10IAfVNxXwALgh8eFQrzmmjMa5HwcwbeGcBtADaIyLrUYw8BGA3gRREZAGAHgJ4FGSEVCvOaTA3AvBYNP6tQ3gSQroB2fbDDobAwr4n1pVKKeS0SRbuV/o477rDaZ599tq/XPfbYY1Z73759gY2Jjps1a5bVvvnmm3XsXQ7o1969e632kCFDdDxt2rSc3pOi5/2A1rxB9Y4dO6y+VatWhTKmsHArPRGRoziBExE5qmhLKDfeeGPaPnPZIAB8+OGHOp4+fXrBxkTHffzxx1a7W7duOl68eLHV165dOx17rza3fPlyHU+cONHq440YksEsmQD2EuHx48dbfYcPHw5lTGHhGTgRkaM4gRMROYoTOBGRo4qqBn7uuefq+Dvf+U7a53nv6jJy5Egdmze3pfCYW9nbt28f4Ugobrw/k1deeWVEIwkfz8CJiBzFCZyIyFFFVUIxd2V5d/qZV7t74YUXrL7nn3++sAMjIsoBz8CJiBzFCZyIyFGcwImIHCXebeMFPRhv0RQbGS45WmPMa6ysUUp1yP607JjXWKk2rzwDJyJyFCdwIiJHcQInInIUJ3AiIkdxAicichQncCIiR4W9lX4PgI8AnJWK46AYx9I84PdjXjMLcyxB5pZ5zSzyvIa6DlwfVGR1UGtV88WxBCdO4+dYghOn8XMsNpZQiIgcxQmciMhRUU3gkyI6bnU4luDEafwcS3DiNH6OxRBJDZyIiPLHEgoRkaM4gRMROSrUCVxEuojIFhHZJiLDwjx26viTRaRSRDYajzUWkaUisjX1vVEI4ygVkddEZJOIvCci90Y1liAwr9ZYEpNb5tUaSyzzGtoELiK1AEwAcAOANgB6i0ibsI6fMhVAF89jwwCUK6VaAShPtQvtCIAHlFLfBdAJwD2pv4soxpIX5vUEicgt83qCeOZVKRXKF4ArASwx2sMBDA/r+MZxWwDYaLS3AChJxSUAtkQwpgUAyuIwFuaVuWVe3clrmCWUZgA+NtoVqcei1lQptQsAUt+bhHlwEWkB4FIAq6IeS46Y1zQczy3zmkac8hrmBF7dLbyKeg2jiDQAMBfAYKXU/qjHkyPmtRoJyC3zWo245TXMCbwCQKnR/jaAnSEeP53dIlICAKnvlWEcVERqo+ofwgtKqXlRjiVPzKtHQnLLvHrEMa9hTuDvAGglIi1FpA6AXgAWhnj8dBYC6JuK+6KqtlVQIiIAngOwSSk1NsqxBIB5NSQot8yrIbZ5Dbnw3xXAvwFsBzAigg8eZgLYBeAwqs4wBgA4E1WfHm9NfW8cwjiuQtWvo+sBrEt9dY1iLMwrc8u8uptXbqUnInIUd2ISETmKEzgRkaM4gRMROYoTOBGRoziBExE5ihM4EZGjOIETETnq/3Loo+Q112ItAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot\n",
    "for i in range(6):\n",
    "    plt.subplot(2, 3, i+1)   #2 raws, 3 columns, in index i+1\n",
    "    plt.imshow(samples[i][0], cmap='gray')\n",
    "    #we want to show the actual data here, [i][0] to access the first channel, and the color map is set to gray\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "db8d20e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we want to classify these digits so we need a fully connected neural network with one hidden layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c47d9256",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes): #num_classes is the output size\n",
    "        super(NeuralNet, self).__init__()\n",
    "        \n",
    "        #fist layer\n",
    "        self.l1 = nn.Linear(input_size, hidden_size)\n",
    "        \n",
    "        #activation function\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        #another linear layer\n",
    "        self.l2 = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #now we use all above layers\n",
    "        out = self.l1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.l2(out)\n",
    "        \n",
    "        #cross entropy will implement softmax,too, so we don't add softmax here\n",
    "        \n",
    "        return out\n",
    "    \n",
    "\n",
    "model = NeuralNet(input_size, hidden_size, num_classes)\n",
    "\n",
    "#loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1c6b8c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/ 2, step 100/600,loss = 0.4042\n",
      "epoch 1/ 2, step 200/600,loss = 0.3594\n",
      "epoch 1/ 2, step 300/600,loss = 0.2723\n",
      "epoch 1/ 2, step 400/600,loss = 0.2700\n",
      "epoch 1/ 2, step 500/600,loss = 0.3132\n",
      "epoch 1/ 2, step 600/600,loss = 0.3337\n",
      "epoch 2/ 2, step 100/600,loss = 0.1563\n",
      "epoch 2/ 2, step 200/600,loss = 0.1826\n",
      "epoch 2/ 2, step 300/600,loss = 0.0957\n",
      "epoch 2/ 2, step 400/600,loss = 0.0962\n",
      "epoch 2/ 2, step 500/600,loss = 0.1909\n",
      "epoch 2/ 2, step 600/600,loss = 0.0807\n"
     ]
    }
   ],
   "source": [
    "#training loop\n",
    "n_total_steps = len(train_loader)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):    #unpacking, enumerate give us the index\n",
    "        #we have to reshape our images first because this is 100 * 1 * 28 * 28\n",
    "        #but now input size is 784 so our images tensors needs the size 100 * 784\n",
    "        #first the number of batches then images size\n",
    "        images = images.reshape(-1, 28*28).to(device) # and send it to GPU if it's there\n",
    "        \n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        #forward\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels) #predicted output and actual labels\n",
    "        \n",
    "        \n",
    "        #backward\n",
    "        \n",
    "        #empty the grads\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        #update\n",
    "        optimizer.step()\n",
    "        \n",
    "        if(i+1)%100 == 0:\n",
    "            print(f'epoch {epoch+1}/ {num_epochs}, step {i+1}/{n_total_steps},loss = {loss.item():.4f}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cce4682c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "accuracy = 98.0\n",
      "200\n",
      "accuracy = 98.5\n",
      "300\n",
      "accuracy = 96.66666666666667\n",
      "400\n",
      "accuracy = 96.0\n",
      "500\n",
      "accuracy = 95.8\n",
      "600\n",
      "accuracy = 95.0\n",
      "700\n",
      "accuracy = 95.0\n",
      "800\n",
      "accuracy = 95.0\n",
      "900\n",
      "accuracy = 95.11111111111111\n",
      "1000\n",
      "accuracy = 94.8\n",
      "1100\n",
      "accuracy = 94.54545454545455\n",
      "1200\n",
      "accuracy = 94.08333333333333\n",
      "1300\n",
      "accuracy = 93.61538461538461\n",
      "1400\n",
      "accuracy = 93.57142857142857\n",
      "1500\n",
      "accuracy = 93.6\n",
      "1600\n",
      "accuracy = 93.375\n",
      "1700\n",
      "accuracy = 93.41176470588235\n",
      "1800\n",
      "accuracy = 93.33333333333333\n",
      "1900\n",
      "accuracy = 93.21052631578948\n",
      "2000\n",
      "accuracy = 93.05\n",
      "2100\n",
      "accuracy = 92.80952380952381\n",
      "2200\n",
      "accuracy = 92.63636363636364\n",
      "2300\n",
      "accuracy = 92.73913043478261\n",
      "2400\n",
      "accuracy = 92.79166666666667\n",
      "2500\n",
      "accuracy = 92.84\n",
      "2600\n",
      "accuracy = 93.03846153846153\n",
      "2700\n",
      "accuracy = 93.18518518518519\n",
      "2800\n",
      "accuracy = 93.28571428571429\n",
      "2900\n",
      "accuracy = 93.34482758620689\n",
      "3000\n",
      "accuracy = 93.26666666666667\n",
      "3100\n",
      "accuracy = 93.35483870967742\n",
      "3200\n",
      "accuracy = 93.375\n",
      "3300\n",
      "accuracy = 93.42424242424242\n",
      "3400\n",
      "accuracy = 93.52941176470588\n",
      "3500\n",
      "accuracy = 93.62857142857143\n",
      "3600\n",
      "accuracy = 93.52777777777777\n",
      "3700\n",
      "accuracy = 93.56756756756756\n",
      "3800\n",
      "accuracy = 93.39473684210526\n",
      "3900\n",
      "accuracy = 93.28205128205128\n",
      "4000\n",
      "accuracy = 93.2\n",
      "4100\n",
      "accuracy = 93.17073170731707\n",
      "4200\n",
      "accuracy = 93.0952380952381\n",
      "4300\n",
      "accuracy = 93.02325581395348\n",
      "4400\n",
      "accuracy = 93.04545454545455\n",
      "4500\n",
      "accuracy = 93.04444444444445\n",
      "4600\n",
      "accuracy = 93.04347826086956\n",
      "4700\n",
      "accuracy = 93.08510638297872\n",
      "4800\n",
      "accuracy = 93.125\n",
      "4900\n",
      "accuracy = 93.0\n",
      "5000\n",
      "accuracy = 92.98\n",
      "5100\n",
      "accuracy = 93.07843137254902\n",
      "5200\n",
      "accuracy = 93.13461538461539\n",
      "5300\n",
      "accuracy = 93.20754716981132\n",
      "5400\n",
      "accuracy = 93.31481481481481\n",
      "5500\n",
      "accuracy = 93.4\n",
      "5600\n",
      "accuracy = 93.48214285714286\n",
      "5700\n",
      "accuracy = 93.47368421052632\n",
      "5800\n",
      "accuracy = 93.51724137931035\n",
      "5900\n",
      "accuracy = 93.54237288135593\n",
      "6000\n",
      "accuracy = 93.51666666666667\n",
      "6100\n",
      "accuracy = 93.44262295081967\n",
      "6200\n",
      "accuracy = 93.46774193548387\n",
      "6300\n",
      "accuracy = 93.57142857142857\n",
      "6400\n",
      "accuracy = 93.640625\n",
      "6500\n",
      "accuracy = 93.72307692307692\n",
      "6600\n",
      "accuracy = 93.68181818181819\n",
      "6700\n",
      "accuracy = 93.70149253731343\n",
      "6800\n",
      "accuracy = 93.70588235294117\n",
      "6900\n",
      "accuracy = 93.78260869565217\n",
      "7000\n",
      "accuracy = 93.85714285714286\n",
      "7100\n",
      "accuracy = 93.92957746478874\n",
      "7200\n",
      "accuracy = 94.0\n",
      "7300\n",
      "accuracy = 94.04109589041096\n",
      "7400\n",
      "accuracy = 94.10810810810811\n",
      "7500\n",
      "accuracy = 94.10666666666667\n",
      "7600\n",
      "accuracy = 94.14473684210526\n",
      "7700\n",
      "accuracy = 94.20779220779221\n",
      "7800\n",
      "accuracy = 94.28205128205128\n",
      "7900\n",
      "accuracy = 94.29113924050633\n",
      "8000\n",
      "accuracy = 94.3\n",
      "8100\n",
      "accuracy = 94.30864197530865\n",
      "8200\n",
      "accuracy = 94.36585365853658\n",
      "8300\n",
      "accuracy = 94.36144578313252\n",
      "8400\n",
      "accuracy = 94.38095238095238\n",
      "8500\n",
      "accuracy = 94.36470588235294\n",
      "8600\n",
      "accuracy = 94.3953488372093\n",
      "8700\n",
      "accuracy = 94.45977011494253\n",
      "8800\n",
      "accuracy = 94.52272727272727\n",
      "8900\n",
      "accuracy = 94.58426966292134\n",
      "9000\n",
      "accuracy = 94.64444444444445\n",
      "9100\n",
      "accuracy = 94.62637362637362\n",
      "9200\n",
      "accuracy = 94.68478260869566\n",
      "9300\n",
      "accuracy = 94.72043010752688\n",
      "9400\n",
      "accuracy = 94.76595744680851\n",
      "9500\n",
      "accuracy = 94.8\n",
      "9600\n",
      "accuracy = 94.83333333333333\n",
      "9700\n",
      "accuracy = 94.79381443298969\n",
      "9800\n",
      "accuracy = 94.6734693877551\n",
      "9900\n",
      "accuracy = 94.66666666666667\n",
      "10000\n",
      "accuracy = 94.64\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "with torch.no_grad():\n",
    "    n_correct = 0 \n",
    "    n_samples = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, 28*28).to(device) # and send it to GPU if it's there\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        \n",
    "        #torch.max returns value and index but we need index\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        n_samples += labels.shape[0]  #number of sample and current batch = should be 100\n",
    "        print(n_samples)\n",
    "        n_correct += (predictions == labels).sum().item()\n",
    "        \n",
    "        \n",
    "        acc = 100.0 * n_correct / n_samples #accuracy\n",
    "        print(f'accuracy = {acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eff28c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
